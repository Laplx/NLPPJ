{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELMo output keys: dict_keys(['lstm_outputs1', 'lstm_outputs2', 'word_emb', 'elmo', 'sequence_len', 'default'])\n",
      "ELMo output keys: dict_keys(['lstm_outputs1', 'lstm_outputs2', 'word_emb', 'elmo', 'sequence_len', 'default'])\n",
      "Valid sentence is: s2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# 设置镜像源（清华源）\n",
    "os.environ['TFHUB_MODEL_LOAD_FORMAT'] = 'COMPRESSED'\n",
    "os.environ['TFHUB_CACHE_DIR'] = '/tmp/tfhub'\n",
    "\n",
    "# 使用国内镜像地址\n",
    "model_url = \"https://hub.tensorflow.google.cn/google/elmo/3\"\n",
    "elmo = hub.load(model_url).signatures[\"default\"]\n",
    "\n",
    "def get_elmo_embedding(text):\n",
    "    # 输入必须是 list of strings\n",
    "    result = elmo(tf.constant([text]))\n",
    "    # 打印 keys 来确认\n",
    "    print(\"ELMo output keys:\", result.keys())\n",
    "    # 使用 'elmo' 键来获取最终向量表示\n",
    "    embeddings = result[\"elmo\"]\n",
    "    return tf.reduce_mean(embeddings, axis=1).numpy()[0]\n",
    "\n",
    "def evaluate_sentence_pair(s1, s2):\n",
    "    emb1 = get_elmo_embedding(s1)\n",
    "    emb2 = get_elmo_embedding(s2)\n",
    "    \n",
    "    score1 = np.linalg.norm(emb1)\n",
    "    score2 = np.linalg.norm(emb2)\n",
    "    \n",
    "    return 0 if score1 > score2 else 1\n",
    "\n",
    "# 测试\n",
    "s1 = \"The cat sat on the mat.\"\n",
    "s2 = \"The mat sat on the cat.\"\n",
    "pred = evaluate_sentence_pair(s1, s2)\n",
    "print(\"Valid sentence is:\", \"s1\" if pred == 0 else \"s2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ck\\AppData\\Local\\Temp\\ipykernel_22412\\3301223317.py:51: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  similarity = np.dot(sentence_emb, reason_emb) / (np.linalg.norm(sentence_emb) * np.linalg.norm(reason_emb))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELMo model explanation selection accuracy: 0.36\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "\n",
    "# 设置镜像源\n",
    "os.environ['TFHUB_MODEL_LOAD_FORMAT'] = 'COMPRESSED'\n",
    "os.environ['TFHUB_CACHE_DIR'] = '/tmp/tfhub'\n",
    "\n",
    "# 使用国内镜像地址加载 ELMo 模型\n",
    "model_url = \"https://hub.tensorflow.google.cn/google/elmo/3\"\n",
    "elmo = hub.load(model_url).signatures[\"default\"]\n",
    "\n",
    "# 获取句向量（平均池化）\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.strip()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s.,!?']\", '', text)  # 只保留基础英文字符\n",
    "    return text\n",
    "\n",
    "def get_elmo_embedding(text):\n",
    "    cleaned = clean_text(text)\n",
    "    if cleaned == \"\":\n",
    "        return np.zeros(1024)  # ELMo 输出维度是 1024\n",
    "    try:\n",
    "        embeddings = elmo(tf.constant([cleaned]))[\"elmo\"]\n",
    "        return tf.reduce_mean(embeddings, axis=1).numpy()[0]\n",
    "    except Exception as e:\n",
    "        print(f\"ELMo embedding error for text: '{text}'\\n{e}\")\n",
    "        return np.zeros(1024)\n",
    "\n",
    "\n",
    "# 判断哪句更合理（返回 0 表示 sentence0 更合理，1 表示 sentence1 更合理）\n",
    "def evaluate_sentence_pair(s1, s2):\n",
    "    emb1 = get_elmo_embedding(s1)\n",
    "    emb2 = get_elmo_embedding(s2)\n",
    "    score1 = np.linalg.norm(emb1)\n",
    "    score2 = np.linalg.norm(emb2)\n",
    "    return 0 if score1 > score2 else 1\n",
    "\n",
    "# 给定错误句子和解释选项，选择与其最相似的解释（返回 'A'/'B'/'C'）\n",
    "def select_reason(false_sentence, options):\n",
    "    sentence_emb = get_elmo_embedding(false_sentence)\n",
    "    scores = {}\n",
    "    for key in ['A', 'B', 'C']:\n",
    "        reason_emb = get_elmo_embedding(options[key])\n",
    "        similarity = np.dot(sentence_emb, reason_emb) / (np.linalg.norm(sentence_emb) * np.linalg.norm(reason_emb))\n",
    "        scores[key] = similarity\n",
    "    return max(scores, key=scores.get)\n",
    "\n",
    "# 读取数据文件\n",
    "def load_jsonl(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        return [json.loads(line.strip()) for line in f]\n",
    "\n",
    "# 加载数据（注意替换文件名）\n",
    "data = load_jsonl('dataset.jsonl')\n",
    "\n",
    "# # 评估句子选择准确率\n",
    "# correct_sentence = 0\n",
    "# for item in data:\n",
    "#     pred = evaluate_sentence_pair(item[\"sentence0\"], item[\"sentence1\"])\n",
    "#     if pred != item[\"false\"]:\n",
    "#         correct_sentence += 1\n",
    "# sentence_accuracy = correct_sentence / len(data)\n",
    "# print(f\"ELMo model sentence selection accuracy: {sentence_accuracy:.2f}\")\n",
    "\n",
    "# 评估解释选择准确率\n",
    "correct_reason = 0\n",
    "for item in data:\n",
    "    false_sentence = item[\"sentence0\"] if item[\"false\"] == 0 else item[\"sentence1\"]\n",
    "    pred_reason = select_reason(false_sentence, {\"A\": item[\"A\"], \"B\": item[\"B\"], \"C\": item[\"C\"]})\n",
    "    if pred_reason == item[\"reason\"]:\n",
    "        correct_reason += 1\n",
    "reason_accuracy = correct_reason / len(data)\n",
    "print(f\"ELMo model explanation selection accuracy: {reason_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELMo model explanation selection (concatenated form) accuracy: 0.40\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "import re\n",
    "\n",
    "# 配置 ELMo 模型加载\n",
    "os.environ['TFHUB_MODEL_LOAD_FORMAT'] = 'COMPRESSED'\n",
    "os.environ['TFHUB_CACHE_DIR'] = '/tmp/tfhub'\n",
    "model_url = \"https://hub.tensorflow.google.cn/google/elmo/3\"\n",
    "elmo = hub.load(model_url).signatures[\"default\"]\n",
    "\n",
    "# 清洗文本\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.strip()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s.,!?']\", '', text)\n",
    "    return text\n",
    "\n",
    "# 获取ELMo向量（平均池化）\n",
    "def get_elmo_embedding(text):\n",
    "    text = clean_text(text)\n",
    "    if text == \"\":\n",
    "        return np.zeros(1024)\n",
    "    try:\n",
    "        embeddings = elmo(tf.constant([text]))[\"elmo\"]\n",
    "        return tf.reduce_mean(embeddings, axis=1).numpy()[0]\n",
    "    except Exception as e:\n",
    "        print(f\"Embedding error for: {text}\\n{e}\")\n",
    "        return np.zeros(1024)\n",
    "\n",
    "# 加载 jsonl 数据\n",
    "def load_jsonl(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        return [json.loads(line.strip()) for line in f]\n",
    "\n",
    "# 构造三条句子 + label\n",
    "def build_reason_sentences(data):\n",
    "    all_sentences = []\n",
    "    all_labels = []\n",
    "    for item in data:\n",
    "        false_sent = item[\"sentence0\"] if item[\"false\"] == 0 else item[\"sentence1\"]\n",
    "        reasons = [\n",
    "            f\"{false_sent} is against common sense because {item['A']}.\",\n",
    "            f\"{false_sent} is against common sense because {item['B']}.\",\n",
    "            f\"{false_sent} is against common sense because {item['C']}.\"\n",
    "        ]\n",
    "        label = ord(item[\"reason\"]) - ord(\"A\")  # A=0, B=1, C=2\n",
    "        all_sentences.append(reasons)\n",
    "        all_labels.append(label)\n",
    "    return all_sentences, all_labels\n",
    "\n",
    "# 模型推理\n",
    "def evaluate_reason_selection(all_sentences, all_labels):\n",
    "    correct = 0\n",
    "    for reasons, label in zip(all_sentences, all_labels):\n",
    "        sims = []\n",
    "        for reason in reasons:\n",
    "            emb = get_elmo_embedding(reason)\n",
    "            sim = np.linalg.norm(emb)\n",
    "            sims.append(sim)\n",
    "        pred = np.argmax(sims)\n",
    "        if pred == label:\n",
    "            correct += 1\n",
    "    return correct / len(all_labels)\n",
    "\n",
    "# 主程序\n",
    "data = load_jsonl(\"dataset.jsonl\")\n",
    "reason_sentences, reason_labels = build_reason_sentences(data)\n",
    "accuracy = evaluate_reason_selection(reason_sentences, reason_labels)\n",
    "print(f\"ELMo model explanation sel+ection (concatenated form) accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELMo model accuracy on your dataset: 0.62\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "\n",
    "# 设置镜像源（清华源）\n",
    "os.environ['TFHUB_MODEL_LOAD_FORMAT'] = 'COMPRESSED'\n",
    "os.environ['TFHUB_CACHE_DIR'] = '/tmp/tfhub'\n",
    "\n",
    "# 使用国内镜像地址\n",
    "model_url = \"https://hub.tensorflow.google.cn/google/elmo/3\"\n",
    "elmo = hub.load(model_url).signatures[\"default\"]\n",
    "\n",
    "\n",
    "# 获取句向量（平均池化）\n",
    "def get_elmo_embedding(text):\n",
    "    embeddings = elmo(tf.constant([text]))[\"elmo\"]\n",
    "    return tf.reduce_mean(embeddings, axis=1).numpy()[0]\n",
    "\n",
    "# 判断哪句更合理（返回 0 表示 sentence0 更合理，1 表示 sentence1 更合理）\n",
    "def evaluate_sentence_pair(s1, s2):\n",
    "    emb1 = get_elmo_embedding(s1)\n",
    "    emb2 = get_elmo_embedding(s2)\n",
    "    score1 = np.linalg.norm(emb1)\n",
    "    score2 = np.linalg.norm(emb2)\n",
    "    return 0 if score1 > score2 else 1\n",
    "\n",
    "def load_jsonl(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        return [json.loads(line.strip()) for line in f]\n",
    "# 数据列表（可以从文件中读取）\n",
    "data = load_jsonl('dataset.jsonl')\n",
    "\n",
    "# 评估准确率\n",
    "correct = 0\n",
    "for item in data:\n",
    "    pred = evaluate_sentence_pair(item[\"sentence0\"], item[\"sentence1\"])\n",
    "    # 如果模型选择的句子不是错误的那一句，则为正确\n",
    "    if pred != item[\"false\"]:\n",
    "        correct += 1\n",
    "\n",
    "accuracy = correct / len(data)\n",
    "print(f\"ELMo model accuracy on your dataset: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELMo + cosine similarity accuracy: 0.50\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "# 获取句向量（平均池化）\n",
    "def get_elmo_embedding(text):\n",
    "    embeddings = elmo(tf.constant([text]))[\"elmo\"]  # shape: [1, timesteps, 1024]\n",
    "    return tf.reduce_mean(embeddings, axis=1).numpy()[0]  # shape: [1024]\n",
    "\n",
    "# 使用句向量与零向量之间的余弦相似度作为“自然程度”判断\n",
    "def evaluate_sentence_pair(s1, s2):\n",
    "    emb1 = get_elmo_embedding(s1)\n",
    "    emb2 = get_elmo_embedding(s2)\n",
    "    # 比较哪个向量更“接近”中心语言空间方向（原点除外的方向）\n",
    "    sim1 = cosine_similarity([emb1], [[0]*1024])[0][0]\n",
    "    sim2 = cosine_similarity([emb2], [[0]*1024])[0][0]\n",
    "    return 0 if sim1 > sim2 else 1\n",
    "\n",
    "# 加载数据集\n",
    "def load_jsonl(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        return [json.loads(line.strip()) for line in f]\n",
    "\n",
    "data = load_jsonl('dataset.jsonl')\n",
    "\n",
    "# 评估准确率\n",
    "correct = 0\n",
    "for item in data:\n",
    "    pred = evaluate_sentence_pair(item[\"sentence0\"], item[\"sentence1\"])\n",
    "    if pred != item[\"false\"]:\n",
    "        correct += 1\n",
    "\n",
    "accuracy = correct / len(data)\n",
    "print(f\"ELMo + cosine similarity accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELMo forward/backward consistency accuracy: 0.58\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "\n",
    "# 获取 forward 和 backward 的平均向量\n",
    "def get_forward_backward_embeddings(text):\n",
    "    elmo_output = elmo(tf.constant([text]))\n",
    "    fw = elmo_output[\"lstm_outputs1\"][:, :, :512]  # forward LSTM hidden state\n",
    "    bw = elmo_output[\"lstm_outputs1\"][:, :, 512:]  # backward LSTM hidden state\n",
    "    # 平均池化\n",
    "    fw_avg = tf.reduce_mean(fw, axis=1).numpy()[0]\n",
    "    bw_avg = tf.reduce_mean(bw, axis=1).numpy()[0]\n",
    "    return fw_avg, bw_avg\n",
    "\n",
    "# 判断哪句更合理（forward 与 backward 的余弦夹角更小者更合理）\n",
    "def evaluate_sentence_pair(s1, s2):\n",
    "    fw1, bw1 = get_forward_backward_embeddings(s1)\n",
    "    fw2, bw2 = get_forward_backward_embeddings(s2)\n",
    "    sim1 = cosine_similarity([fw1], [bw1])[0][0]\n",
    "    sim2 = cosine_similarity([fw2], [bw2])[0][0]\n",
    "    return 0 if sim1 > sim2 else 1  # 相似度越高，forward/backward 越一致\n",
    "\n",
    "# 加载数据\n",
    "def load_jsonl(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        return [json.loads(line.strip()) for line in f]\n",
    "\n",
    "# 加载数据并评估\n",
    "data = load_jsonl('dataset.jsonl')\n",
    "\n",
    "correct = 0\n",
    "for item in data:\n",
    "    pred = evaluate_sentence_pair(item[\"sentence0\"], item[\"sentence1\"])\n",
    "    if pred != item[\"false\"]:\n",
    "        correct += 1\n",
    "\n",
    "accuracy = correct / len(data)\n",
    "print(f\"ELMo forward/backward consistency accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELMo forward/backward Euclidean distance accuracy: 0.45\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "\n",
    "\n",
    "# 获取 forward 和 backward 的平均向量\n",
    "def get_forward_backward_embeddings(text):\n",
    "    elmo_output = elmo(tf.constant([text]))\n",
    "    fw = elmo_output[\"lstm_outputs1\"][:, :, :512]  # forward LSTM hidden state\n",
    "    bw = elmo_output[\"lstm_outputs1\"][:, :, 512:]  # backward LSTM hidden state\n",
    "    fw_avg = tf.reduce_mean(fw, axis=1).numpy()[0]\n",
    "    bw_avg = tf.reduce_mean(bw, axis=1).numpy()[0]\n",
    "    return fw_avg, bw_avg\n",
    "\n",
    "# 判断哪句更合理（forward 和 backward 的欧几里得距离更小）\n",
    "def evaluate_sentence_pair(s1, s2):\n",
    "    fw1, bw1 = get_forward_backward_embeddings(s1)\n",
    "    fw2, bw2 = get_forward_backward_embeddings(s2)\n",
    "    dist1 = np.linalg.norm(fw1 - bw1)\n",
    "    dist2 = np.linalg.norm(fw2 - bw2)\n",
    "    return 0 if dist1 < dist2 else 1  # 距离越小越好\n",
    "\n",
    "# 加载数据\n",
    "def load_jsonl(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        return [json.loads(line.strip()) for line in f]\n",
    "\n",
    "# 加载数据并评估\n",
    "data = load_jsonl('dataset.jsonl')\n",
    "\n",
    "correct = 0\n",
    "for item in data:\n",
    "    pred = evaluate_sentence_pair(item[\"sentence0\"], item[\"sentence1\"])\n",
    "    if pred != item[\"false\"]:\n",
    "        correct += 1\n",
    "\n",
    "accuracy = correct / len(data)\n",
    "print(f\"ELMo forward/backward Euclidean distance accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import requests\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def get_elmo_embedding(text):\n",
    "    embeddings = elmo(tf.constant([text]))[\"elmo\"]\n",
    "    return tf.reduce_mean(embeddings, axis=1).numpy()[0]\n",
    "\n",
    "# 从 ConceptNet 获取所有关系\n",
    "def get_all_conceptnet_relations(entities):\n",
    "    relations = []\n",
    "    if len(entities) < 2:\n",
    "        return relations\n",
    "    for i in range(len(entities)):\n",
    "        for j in range(i + 1, len(entities)):\n",
    "            start_node = entities[i].lower().replace(\" \", \"_\")\n",
    "            end_node = entities[j].lower().replace(\" \", \"_\")\n",
    "            url = f\"http://api.conceptnet.io/query?node1=/c/en/{start_node}&node2=/c/en/{end_node}&rel=/r/RelatedTo\"\n",
    "            try:\n",
    "                response = requests.get(url, timeout=5)\n",
    "                data = response.json()\n",
    "                if data[\"edges\"]:\n",
    "                    for edge in data[\"edges\"]:\n",
    "                        relation = edge[\"rel\"][\"label\"]\n",
    "                        relations.append(f\"{end_node} is related to {start_node} via {relation}.\")\n",
    "            except Exception as e:\n",
    "                print(f\"ConceptNet error for {start_node} and {end_node}: {e}\")\n",
    "                relations.append(f\"{end_node} is related to {start_node} via related to.\")\n",
    "    return relations\n",
    "\n",
    "# 实体识别并编码所有知识\n",
    "def encode_with_all_knowledge(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    entities = [ent.text for ent in doc.ents if ent.label_ in [\"PERSON\", \"ORG\", \"GPE\", \"PRODUCT\", \"FAC\", \"EVENT\", \"WORK_OF_ART\"]]\n",
    "    if not entities:\n",
    "        return enhanced_sentence\n",
    "    knowledge_sentences = get_all_conceptnet_relations(entities)\n",
    "    if not knowledge_sentences:\n",
    "        return sentence\n",
    "\n",
    "    enhanced_sentence = sentence\n",
    "    for knowledge in knowledge_sentences:\n",
    "        enhanced_sentence += f\" {knowledge}\"\n",
    "    return enhanced_sentence\n",
    "\n",
    "# 加载 jsonl 数据\n",
    "def load_jsonl(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        return [json.loads(line.strip()) for line in f]\n",
    "\n",
    "# 构建理由句子并添加知识编码\n",
    "def build_reason_sentences(data):\n",
    "    all_sentences = []\n",
    "    all_labels = []\n",
    "    for item in data:\n",
    "        false_sent = item[\"sentence0\"] if item[\"false\"] == 0 else item[\"sentence1\"]\n",
    "        # 编码所有知识\n",
    "        false_sent_encoded = encode_with_all_knowledge(false_sent)\n",
    "        reasons = [\n",
    "            f\"{false_sent_encoded} is against common sense because {item['A']}.\",\n",
    "            f\"{false_sent_encoded} is against common sense because {item['B']}.\",\n",
    "            f\"{false_sent_encoded} is against common sense because {item['C']}.\"\n",
    "        ]\n",
    "        label = ord(item[\"reason\"]) - ord(\"A\")  # A=0, B=1, C=2\n",
    "        all_sentences.append(reasons)\n",
    "        all_labels.append(label)\n",
    "    return all_sentences, all_labels\n",
    "\n",
    "# 评估理由选择（基于嵌入范数）\n",
    "def evaluate_reason_selection(all_sentences, all_labels):\n",
    "    correct = 0\n",
    "    for reasons, label in zip(all_sentences, all_labels):\n",
    "        sims = []\n",
    "        for reason in reasons:\n",
    "            emb = get_elmo_embedding(reason)\n",
    "            # 使用嵌入的范数作为得分\n",
    "            sim = np.linalg.norm(emb)\n",
    "            sims.append(sim)\n",
    "        pred = np.argmax(sims)\n",
    "        if pred == label:\n",
    "            correct += 1\n",
    "    return correct / len(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "data = load_jsonl('dataset.jsonl')\n",
    "\n",
    "# 构建知识编码后的句子\n",
    "reason_sentences, reason_labels = build_reason_sentences(data)\n",
    "\n",
    "# 评估\n",
    "accuracy = evaluate_reason_selection(reason_sentences, reason_labels)\n",
    "print(f\"ELMo model (with all knowledge enhancement) explanation selection accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sentence_pair(s1, s2):\n",
    "    # 增强知识\n",
    "    s1_encoded = encode_with_all_knowledge(s1)\n",
    "    s2_encoded = encode_with_all_knowledge(s2)\n",
    "    # 获取嵌入\n",
    "    emb1 = get_elmo_embedding(s1_encoded)\n",
    "    emb2 = get_elmo_embedding(s2_encoded)\n",
    "    # 使用嵌入范数作为得分\n",
    "    score1 = np.linalg.norm(emb1)\n",
    "    score2 = np.linalg.norm(emb2)\n",
    "    return 0 if score1 > score2 else 1  # 得分越高越合理，返回 0 表示 sentence0 更合理\n",
    "\n",
    "correct_sentence = 0\n",
    "for item in data:\n",
    "    pred = evaluate_sentence_pair(item[\"sentence0\"], item[\"sentence1\"])\n",
    "    # 如果模型选择的句子不是错误的那一句，则为正确\n",
    "    if pred != item[\"false\"]:\n",
    "        correct_sentence += 1\n",
    "sentence_accuracy = correct_sentence / len(data)\n",
    "print(f\"ELMo model (with knowledge enhancement) sentence selection accuracy: {sentence_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
